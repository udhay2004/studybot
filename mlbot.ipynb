{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6f668e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA, LLMChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "import gradio as gr\n",
    "from langchain.prompts import PromptTemplate\n",
    "import os\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3d3f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv_path = r\"C:\\Users\\udhay\\Downloads\\.env\"\n",
    "load_dotenv(dotenv_path)\n",
    "openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not openai_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY not found in the .env file!\")\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98034b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded and split 560 document chunks.\n"
     ]
    }
   ],
   "source": [
    "pdf_path = r\"C:\\Users\\udhay\\Downloads\\thebook.pdf\"  \n",
    "loader = PyPDFLoader(pdf_path)\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "print(f\"Loaded and split {len(texts)} document chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "eb5f43f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_documents(texts, embeddings)\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "593579c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_template = \"\"\"\n",
    "Based on the following context, create a quiz with 3 multiple-choice questions about {query}. \n",
    "For each question, provide 4 options and indicate the correct answer.\n",
    "\n",
    "Context: {context}\n",
    "\"\"\"\n",
    "\n",
    "guide_template = \"\"\"\n",
    "Based on the following context, create a concise study guide about {query}.\n",
    "Include key definitions, main concepts, and important relationships.\n",
    "Format it with clear sections and bullet points for readability.\n",
    "\n",
    "Context: {context}\n",
    "\"\"\"\n",
    "\n",
    "flashcard_template = \"\"\"\n",
    "Based on the following context, create 5 flashcards about {query}.\n",
    "Each flashcard should have a question on one side and the answer on the other.\n",
    "Format as:\n",
    "\n",
    "Q: [Question]\n",
    "A: [Answer]\n",
    "\n",
    "Context: {context}\n",
    "\"\"\"\n",
    "\n",
    "practice_template = \"\"\"\n",
    "Based on the following context, create 2 practice problems about {query}.\n",
    "These should be application-based problems that test understanding rather than recall.\n",
    "Provide detailed solutions to each problem.\n",
    "\n",
    "Context: {context}\n",
    "\"\"\"\n",
    "\n",
    "# Create LLM\n",
    "llm = ChatOpenAI(temperature=0.7)\n",
    "\n",
    "# Create chains for different study tools\n",
    "quiz_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=PromptTemplate(template=quiz_template, input_variables=[\"query\", \"context\"]),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "guide_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=PromptTemplate(template=guide_template, input_variables=[\"query\", \"context\"]),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "flashcard_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=PromptTemplate(template=flashcard_template, input_variables=[\"query\", \"context\"]),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "practice_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=PromptTemplate(template=practice_template, input_variables=[\"query\", \"context\"]),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Create standard QA chain\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=ChatOpenAI(temperature=0),\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\n",
    "        \"document_variable_name\": \"context\"\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b2dd6e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_study_tools(input_text, tool_type=\"qa\"):\n",
    "    # Get relevant documents\n",
    "    if tool_type != \"qa\":\n",
    "        docs = retriever.get_relevant_documents(input_text)\n",
    "        context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "    \n",
    "    if tool_type == \"qa\":\n",
    "        result = qa.invoke({\"query\": input_text})\n",
    "        return result['result']\n",
    "    elif tool_type == \"quiz\":\n",
    "        result = quiz_chain.invoke({\"query\": input_text, \"context\": context})\n",
    "        return result['text']\n",
    "    elif tool_type == \"guide\":\n",
    "        result = guide_chain.invoke({\"query\": input_text, \"context\": context})\n",
    "        return result['text']\n",
    "    elif tool_type == \"flashcard\":\n",
    "        result = flashcard_chain.invoke({\"query\": input_text, \"context\": context})\n",
    "        return result['text']\n",
    "    elif tool_type == \"practice\":\n",
    "        result = practice_chain.invoke({\"query\": input_text, \"context\": context})\n",
    "        return result['text']\n",
    "    else:\n",
    "        return \"Invalid tool type. Choose from: qa, quiz, guide, flashcard, practice\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "378253cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7865\n",
      "* Running on public URL: https://bbfa7916a87b45fcf6.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://bbfa7916a87b45fcf6.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Based on the following context, create a concise study guide about linear regression.\n",
      "Include key definitions, main concepts, and important relationships.\n",
      "Format it with clear sections and bullet points for readability.\n",
      "\n",
      "Context: 5.2 Regression 151\n",
      "To minimize the above objective function we ﬁrst compute the gradient.\n",
      "∇J(θ) = θ+\n",
      "m∑\n",
      "i=1\n",
      "exp\n",
      "(⣨\n",
      "−yiˆφ(xi),θ\n",
      "⟩)\n",
      "1 + exp\n",
      "(⣨\n",
      "−yiˆφ(xi),θ\n",
      "⟩)(−yiˆφ(xi))\n",
      "= θ+\n",
      "m∑\n",
      "i=1\n",
      "(p(yi|xi,θ) −1)yiˆφ(xi).\n",
      "Notice that the second term of the gradient vanishes whenever p(yi|xi,θ) =\n",
      "1. Therefore, one way to interpret logistic regression is to view it as a method\n",
      "to maximize p(yi|xi,θ) for each point ( xi,yi) in the training set. Since the\n",
      "objective function of logistic regression is twice diﬀerentiable one can also\n",
      "compute its Hessian\n",
      "∇2J(θ) = I−\n",
      "m∑\n",
      "i=1\n",
      "p(yi|xi,θ)(1 −p(yi|xi,θ))ˆφ(xi)ˆφ(xi)⊤,\n",
      "where we used y2\n",
      "i = 1. The Hessian can be used in the Newton method\n",
      "(Section 3.2.6) to obtain the optimal parameter θ.\n",
      "5.2 Regression\n",
      "5.2.1 Conditionally Normal Models\n",
      "ﬁxed variance\n",
      "5.2.2 Posterior Distribution\n",
      "integrating out vs. Laplace approximation, eﬃcient estimation (sparse greedy)\n",
      "5.2.3 Heteroscedastic Estimation\n",
      "explain that we have two parameters. not too many details (do that as an\n",
      "\n",
      "when attempting to classify webpages, y might be a permutation, when\n",
      "attempting to match objects, to perform collaborative ﬁltering, or to rank\n",
      "documents in a retrieval setting. Equally well, y might be an annotation of\n",
      "a text, when performing named entity recognition. Each of those problems\n",
      "has its own properties in terms of the set of y which we might consider\n",
      "admissible, or how to search this space. We will discuss a number of those\n",
      "problems in Chapter ??.\n",
      "Regression is another prototypical application. Here the goal is to esti-\n",
      "mate a real-valued variable y∈R given a pattern x(see e.g. Figure 1.7). For\n",
      "instance, we might want to estimate the value of a stock the next day, the\n",
      "yield of a semiconductor fab given the current process, the iron content of\n",
      "ore given mass spectroscopy measurements, or the heart rate of an athlete,\n",
      "given accelerometer data. One of the key issues in which regression problems\n",
      "diﬀer from each other is the choice of a loss. For instance, when estimating\n",
      "\n",
      "1.1 A Taste of Machine Learning 11\n",
      "Fig. 1.7. Regression estimation. We are given a number of instances (indicated by\n",
      "black dots) and would like to ﬁnd some function f mapping the observations X to\n",
      "R such that f(x) is close to the observed values.\n",
      "error we make. For instance, in the problem of assessing the risk of cancer, it\n",
      "makes a signiﬁcant diﬀerence whether we mis-classify an early stage of can-\n",
      "cer as healthy (in which case the patient is likely to die) or as an advanced\n",
      "stage of cancer (in which case the patient is likely to be inconvenienced from\n",
      "overly aggressive treatment).\n",
      "Structured Estimation goes beyond simple multiclass estimation by\n",
      "assuming that the labels yhave some additional structure which can be used\n",
      "in the estimation process. For instance, y might be a path in an ontology,\n",
      "when attempting to classify webpages, y might be a permutation, when\n",
      "attempting to match objects, to perform collaborative ﬁltering, or to rank\n",
      "\n",
      "184 7 Linear Models\n",
      "allow us to draw many useful conclusions:\n",
      "•Whenever |yi −(⟨w,xi⟩+ b)|< ϵ, this implies that ξ+\n",
      "i = ξ−\n",
      "i = α+\n",
      "i =\n",
      "α−\n",
      "i = 0. In other words, points which lie inside the ϵ tube around the\n",
      "hyperplane ⟨w,x⟩+ b do not contribute to the solution thus leading to\n",
      "sparse expansions in terms of α.\n",
      "•If (⟨w,xi⟩+b)−yi >ϵ we haveξ−\n",
      "i >0 and therefore α−\n",
      "i = C\n",
      "m. On the other\n",
      "hand, ξ+ = 0 and α+\n",
      "i = 0. The case yi −(⟨w,xi⟩+ b) > ϵis symmetric\n",
      "and yields ξ−= 0, ξ+\n",
      "i >0, α+\n",
      "i = C\n",
      "m, and α−\n",
      "i = 0.\n",
      "•Finally, if (⟨w,xi⟩+ b) −yi = ϵ we have ξ−\n",
      "i = 0 and 0 ≤α−\n",
      "i ≤C\n",
      "m, while\n",
      "ξ+ = 0 and α+\n",
      "i = 0. Similarly, when yi −(⟨w,xi⟩+ b) = ϵ we obtain\n",
      "ξ+\n",
      "i = 0, 0 ≤α+\n",
      "i ≤C\n",
      "m, ξ−= 0 and α−\n",
      "i = 0.\n",
      "Note that α+\n",
      "i and α−\n",
      "i are never simultaneously non-zero.\n",
      "7.3.1 Incorporating General Loss Functions\n",
      "Using the same reasoning as in Section 7.1.1 we can deduce from (7.52) that\n",
      "the loss function of support vector regression is given by\n",
      "l(w,x,y ) = max(0,|y−⟨w,x⟩|− ϵ). (7.59)\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Based on the following context, create 5 flashcards about linear regression.\n",
      "Each flashcard should have a question on one side and the answer on the other.\n",
      "Format as:\n",
      "\n",
      "Q: [Question]\n",
      "A: [Answer]\n",
      "\n",
      "Context: 5.2 Regression 151\n",
      "To minimize the above objective function we ﬁrst compute the gradient.\n",
      "∇J(θ) = θ+\n",
      "m∑\n",
      "i=1\n",
      "exp\n",
      "(⣨\n",
      "−yiˆφ(xi),θ\n",
      "⟩)\n",
      "1 + exp\n",
      "(⣨\n",
      "−yiˆφ(xi),θ\n",
      "⟩)(−yiˆφ(xi))\n",
      "= θ+\n",
      "m∑\n",
      "i=1\n",
      "(p(yi|xi,θ) −1)yiˆφ(xi).\n",
      "Notice that the second term of the gradient vanishes whenever p(yi|xi,θ) =\n",
      "1. Therefore, one way to interpret logistic regression is to view it as a method\n",
      "to maximize p(yi|xi,θ) for each point ( xi,yi) in the training set. Since the\n",
      "objective function of logistic regression is twice diﬀerentiable one can also\n",
      "compute its Hessian\n",
      "∇2J(θ) = I−\n",
      "m∑\n",
      "i=1\n",
      "p(yi|xi,θ)(1 −p(yi|xi,θ))ˆφ(xi)ˆφ(xi)⊤,\n",
      "where we used y2\n",
      "i = 1. The Hessian can be used in the Newton method\n",
      "(Section 3.2.6) to obtain the optimal parameter θ.\n",
      "5.2 Regression\n",
      "5.2.1 Conditionally Normal Models\n",
      "ﬁxed variance\n",
      "5.2.2 Posterior Distribution\n",
      "integrating out vs. Laplace approximation, eﬃcient estimation (sparse greedy)\n",
      "5.2.3 Heteroscedastic Estimation\n",
      "explain that we have two parameters. not too many details (do that as an\n",
      "\n",
      "when attempting to classify webpages, y might be a permutation, when\n",
      "attempting to match objects, to perform collaborative ﬁltering, or to rank\n",
      "documents in a retrieval setting. Equally well, y might be an annotation of\n",
      "a text, when performing named entity recognition. Each of those problems\n",
      "has its own properties in terms of the set of y which we might consider\n",
      "admissible, or how to search this space. We will discuss a number of those\n",
      "problems in Chapter ??.\n",
      "Regression is another prototypical application. Here the goal is to esti-\n",
      "mate a real-valued variable y∈R given a pattern x(see e.g. Figure 1.7). For\n",
      "instance, we might want to estimate the value of a stock the next day, the\n",
      "yield of a semiconductor fab given the current process, the iron content of\n",
      "ore given mass spectroscopy measurements, or the heart rate of an athlete,\n",
      "given accelerometer data. One of the key issues in which regression problems\n",
      "diﬀer from each other is the choice of a loss. For instance, when estimating\n",
      "\n",
      "1.1 A Taste of Machine Learning 11\n",
      "Fig. 1.7. Regression estimation. We are given a number of instances (indicated by\n",
      "black dots) and would like to ﬁnd some function f mapping the observations X to\n",
      "R such that f(x) is close to the observed values.\n",
      "error we make. For instance, in the problem of assessing the risk of cancer, it\n",
      "makes a signiﬁcant diﬀerence whether we mis-classify an early stage of can-\n",
      "cer as healthy (in which case the patient is likely to die) or as an advanced\n",
      "stage of cancer (in which case the patient is likely to be inconvenienced from\n",
      "overly aggressive treatment).\n",
      "Structured Estimation goes beyond simple multiclass estimation by\n",
      "assuming that the labels yhave some additional structure which can be used\n",
      "in the estimation process. For instance, y might be a path in an ontology,\n",
      "when attempting to classify webpages, y might be a permutation, when\n",
      "attempting to match objects, to perform collaborative ﬁltering, or to rank\n",
      "\n",
      "184 7 Linear Models\n",
      "allow us to draw many useful conclusions:\n",
      "•Whenever |yi −(⟨w,xi⟩+ b)|< ϵ, this implies that ξ+\n",
      "i = ξ−\n",
      "i = α+\n",
      "i =\n",
      "α−\n",
      "i = 0. In other words, points which lie inside the ϵ tube around the\n",
      "hyperplane ⟨w,x⟩+ b do not contribute to the solution thus leading to\n",
      "sparse expansions in terms of α.\n",
      "•If (⟨w,xi⟩+b)−yi >ϵ we haveξ−\n",
      "i >0 and therefore α−\n",
      "i = C\n",
      "m. On the other\n",
      "hand, ξ+ = 0 and α+\n",
      "i = 0. The case yi −(⟨w,xi⟩+ b) > ϵis symmetric\n",
      "and yields ξ−= 0, ξ+\n",
      "i >0, α+\n",
      "i = C\n",
      "m, and α−\n",
      "i = 0.\n",
      "•Finally, if (⟨w,xi⟩+ b) −yi = ϵ we have ξ−\n",
      "i = 0 and 0 ≤α−\n",
      "i ≤C\n",
      "m, while\n",
      "ξ+ = 0 and α+\n",
      "i = 0. Similarly, when yi −(⟨w,xi⟩+ b) = ϵ we obtain\n",
      "ξ+\n",
      "i = 0, 0 ≤α+\n",
      "i ≤C\n",
      "m, ξ−= 0 and α−\n",
      "i = 0.\n",
      "Note that α+\n",
      "i and α−\n",
      "i are never simultaneously non-zero.\n",
      "7.3.1 Incorporating General Loss Functions\n",
      "Using the same reasoning as in Section 7.1.1 we can deduce from (7.52) that\n",
      "the loss function of support vector regression is given by\n",
      "l(w,x,y ) = max(0,|y−⟨w,x⟩|− ϵ). (7.59)\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Based on the following context, create 2 practice problems about linear regression.\n",
      "These should be application-based problems that test understanding rather than recall.\n",
      "Provide detailed solutions to each problem.\n",
      "\n",
      "Context: 5.2 Regression 151\n",
      "To minimize the above objective function we ﬁrst compute the gradient.\n",
      "∇J(θ) = θ+\n",
      "m∑\n",
      "i=1\n",
      "exp\n",
      "(⣨\n",
      "−yiˆφ(xi),θ\n",
      "⟩)\n",
      "1 + exp\n",
      "(⣨\n",
      "−yiˆφ(xi),θ\n",
      "⟩)(−yiˆφ(xi))\n",
      "= θ+\n",
      "m∑\n",
      "i=1\n",
      "(p(yi|xi,θ) −1)yiˆφ(xi).\n",
      "Notice that the second term of the gradient vanishes whenever p(yi|xi,θ) =\n",
      "1. Therefore, one way to interpret logistic regression is to view it as a method\n",
      "to maximize p(yi|xi,θ) for each point ( xi,yi) in the training set. Since the\n",
      "objective function of logistic regression is twice diﬀerentiable one can also\n",
      "compute its Hessian\n",
      "∇2J(θ) = I−\n",
      "m∑\n",
      "i=1\n",
      "p(yi|xi,θ)(1 −p(yi|xi,θ))ˆφ(xi)ˆφ(xi)⊤,\n",
      "where we used y2\n",
      "i = 1. The Hessian can be used in the Newton method\n",
      "(Section 3.2.6) to obtain the optimal parameter θ.\n",
      "5.2 Regression\n",
      "5.2.1 Conditionally Normal Models\n",
      "ﬁxed variance\n",
      "5.2.2 Posterior Distribution\n",
      "integrating out vs. Laplace approximation, eﬃcient estimation (sparse greedy)\n",
      "5.2.3 Heteroscedastic Estimation\n",
      "explain that we have two parameters. not too many details (do that as an\n",
      "\n",
      "when attempting to classify webpages, y might be a permutation, when\n",
      "attempting to match objects, to perform collaborative ﬁltering, or to rank\n",
      "documents in a retrieval setting. Equally well, y might be an annotation of\n",
      "a text, when performing named entity recognition. Each of those problems\n",
      "has its own properties in terms of the set of y which we might consider\n",
      "admissible, or how to search this space. We will discuss a number of those\n",
      "problems in Chapter ??.\n",
      "Regression is another prototypical application. Here the goal is to esti-\n",
      "mate a real-valued variable y∈R given a pattern x(see e.g. Figure 1.7). For\n",
      "instance, we might want to estimate the value of a stock the next day, the\n",
      "yield of a semiconductor fab given the current process, the iron content of\n",
      "ore given mass spectroscopy measurements, or the heart rate of an athlete,\n",
      "given accelerometer data. One of the key issues in which regression problems\n",
      "diﬀer from each other is the choice of a loss. For instance, when estimating\n",
      "\n",
      "1.1 A Taste of Machine Learning 11\n",
      "Fig. 1.7. Regression estimation. We are given a number of instances (indicated by\n",
      "black dots) and would like to ﬁnd some function f mapping the observations X to\n",
      "R such that f(x) is close to the observed values.\n",
      "error we make. For instance, in the problem of assessing the risk of cancer, it\n",
      "makes a signiﬁcant diﬀerence whether we mis-classify an early stage of can-\n",
      "cer as healthy (in which case the patient is likely to die) or as an advanced\n",
      "stage of cancer (in which case the patient is likely to be inconvenienced from\n",
      "overly aggressive treatment).\n",
      "Structured Estimation goes beyond simple multiclass estimation by\n",
      "assuming that the labels yhave some additional structure which can be used\n",
      "in the estimation process. For instance, y might be a path in an ontology,\n",
      "when attempting to classify webpages, y might be a permutation, when\n",
      "attempting to match objects, to perform collaborative ﬁltering, or to rank\n",
      "\n",
      "184 7 Linear Models\n",
      "allow us to draw many useful conclusions:\n",
      "•Whenever |yi −(⟨w,xi⟩+ b)|< ϵ, this implies that ξ+\n",
      "i = ξ−\n",
      "i = α+\n",
      "i =\n",
      "α−\n",
      "i = 0. In other words, points which lie inside the ϵ tube around the\n",
      "hyperplane ⟨w,x⟩+ b do not contribute to the solution thus leading to\n",
      "sparse expansions in terms of α.\n",
      "•If (⟨w,xi⟩+b)−yi >ϵ we haveξ−\n",
      "i >0 and therefore α−\n",
      "i = C\n",
      "m. On the other\n",
      "hand, ξ+ = 0 and α+\n",
      "i = 0. The case yi −(⟨w,xi⟩+ b) > ϵis symmetric\n",
      "and yields ξ−= 0, ξ+\n",
      "i >0, α+\n",
      "i = C\n",
      "m, and α−\n",
      "i = 0.\n",
      "•Finally, if (⟨w,xi⟩+ b) −yi = ϵ we have ξ−\n",
      "i = 0 and 0 ≤α−\n",
      "i ≤C\n",
      "m, while\n",
      "ξ+ = 0 and α+\n",
      "i = 0. Similarly, when yi −(⟨w,xi⟩+ b) = ϵ we obtain\n",
      "ξ+\n",
      "i = 0, 0 ≤α+\n",
      "i ≤C\n",
      "m, ξ−= 0 and α−\n",
      "i = 0.\n",
      "Note that α+\n",
      "i and α−\n",
      "i are never simultaneously non-zero.\n",
      "7.3.1 Incorporating General Loss Functions\n",
      "Using the same reasoning as in Section 7.1.1 we can deduce from (7.52) that\n",
      "the loss function of support vector regression is given by\n",
      "l(w,x,y ) = max(0,|y−⟨w,x⟩|− ϵ). (7.59)\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Based on the following context, create a quiz with 3 multiple-choice questions about svm. \n",
      "For each question, provide 4 options and indicate the correct answer.\n",
      "\n",
      "Context: As before, we can replace (7.21) by a linear penalty for constraint viola-\n",
      "tion in order to recover (7.5). The quantity log p(yi|xi,θ)\n",
      "maxy̸=yip(y|xi,θ) is sometimes\n",
      "called the log-odds ratio, and the above discussion shows that SVMs can\n",
      "be interpreted as maximizing the log-odds ratio in the exponential family.\n",
      "This interpretation will be developed further when we consider extensions of\n",
      "SVMs to tackle multiclass, multilabel, and structured prediction problems.\n",
      "7.1.3 Specialized Algorithms for Training SVMs\n",
      "The main task in training SVMs boils down to solving (7.9). The m×m\n",
      "matrix H is usually dense and cannot be stored in memory. Decomposition\n",
      "methods are designed to overcome these diﬃculties. The basic idea here\n",
      "is to identify and update a small working set B by solving a small sub-\n",
      "problem at every iteration. Formally, let B ⊂{1,...,m }be the working set\n",
      "and αB be the corresponding sub-vector of α. Deﬁne ¯B = {1,...,m }\\ B\n",
      "\n",
      "Berlin, 1982.\n",
      "[Vap95] , The nature of statistical learning theory, Springer, New York, 1995.\n",
      "[Vap98] , Statistical learning theory, John Wiley and Sons, New York, 1998.\n",
      "[vdG00] S. van de Geer, Empirical processes in M-estimation, Cambridge University\n",
      "Press, 2000.\n",
      "[vdVW96] A. W. van der Vaart and J. A. Wellner, Weak convergence and empirical\n",
      "processes, Springer, 1996.\n",
      "[VGS97] V. Vapnik, S. Golowich, and A. J. Smola, Support vector method for func-\n",
      "tion approximation, regression estimation, and signal processing , Advances in\n",
      "Neural Information Processing Systems 9 (Cambridge, MA) (M. C. Mozer,\n",
      "M. I. Jordan, and T. Petsche, eds.), MIT Press, 1997, pp. 281–287.\n",
      "[Voo01] E. Voorhees, Overview of the TRECT 2001 question answering track ,\n",
      "TREC, 2001.\n",
      "[VS04] S. V. N. Vishwanathan and A. J. Smola, Fast kernels for string and\n",
      "tree matching, Kernel Methods in Computational Biology (Cambridge, MA)\n",
      "(B. Sch¨ olkopf, K. Tsuda, and J. P. Vert, eds.), MIT Press, 2004, pp. 113–130.\n",
      "\n",
      "tree matching, Kernel Methods in Computational Biology (Cambridge, MA)\n",
      "(B. Sch¨ olkopf, K. Tsuda, and J. P. Vert, eds.), MIT Press, 2004, pp. 113–130.\n",
      "[VSV07] S. V. N. Vishwanathan, A. J. Smola, and R. Vidal, Binet-Cauchy kernels\n",
      "on dynamical systems and its application to the analysis of dynamic scenes ,\n",
      "International Journal of Computer Vision 73 (2007), no. 1, 95–119.\n",
      "[Wah97] G. Wahba, Support vector machines, reproducing kernel Hilbert spaces and\n",
      "the randomized GACV, Tech. Report 984, Department of Statistics, University\n",
      "of Wisconsin, Madison, 1997.\n",
      "[Wat64] G. S. Watson, Smooth regression analysis, Sankhya A 26 (1964), 359–372.\n",
      "[Wil98] C. K. I. Williams, Prediction with Gaussian processes: From linear regression\n",
      "to linear prediction and beyond , Learning and Inference in Graphical Models\n",
      "(M. I. Jordan, ed.), Kluwer Academic, 1998, pp. 599–621.\n",
      "\n",
      "Introduction to Machine Learning\n",
      "Alex Smola and S.V.N. Vishwanathan\n",
      "Yahoo! Labs\n",
      "Santa Clara\n",
      "–and–\n",
      "Departments of Statistics and Computer Science\n",
      "Purdue University\n",
      "–and–\n",
      "College of Engineering and Computer Science\n",
      "Australian National University\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Based on the following context, create a concise study guide about linear regression.\n",
      "Include key definitions, main concepts, and important relationships.\n",
      "Format it with clear sections and bullet points for readability.\n",
      "\n",
      "Context: 5.2 Regression 151\n",
      "To minimize the above objective function we ﬁrst compute the gradient.\n",
      "∇J(θ) = θ+\n",
      "m∑\n",
      "i=1\n",
      "exp\n",
      "(⣨\n",
      "−yiˆφ(xi),θ\n",
      "⟩)\n",
      "1 + exp\n",
      "(⣨\n",
      "−yiˆφ(xi),θ\n",
      "⟩)(−yiˆφ(xi))\n",
      "= θ+\n",
      "m∑\n",
      "i=1\n",
      "(p(yi|xi,θ) −1)yiˆφ(xi).\n",
      "Notice that the second term of the gradient vanishes whenever p(yi|xi,θ) =\n",
      "1. Therefore, one way to interpret logistic regression is to view it as a method\n",
      "to maximize p(yi|xi,θ) for each point ( xi,yi) in the training set. Since the\n",
      "objective function of logistic regression is twice diﬀerentiable one can also\n",
      "compute its Hessian\n",
      "∇2J(θ) = I−\n",
      "m∑\n",
      "i=1\n",
      "p(yi|xi,θ)(1 −p(yi|xi,θ))ˆφ(xi)ˆφ(xi)⊤,\n",
      "where we used y2\n",
      "i = 1. The Hessian can be used in the Newton method\n",
      "(Section 3.2.6) to obtain the optimal parameter θ.\n",
      "5.2 Regression\n",
      "5.2.1 Conditionally Normal Models\n",
      "ﬁxed variance\n",
      "5.2.2 Posterior Distribution\n",
      "integrating out vs. Laplace approximation, eﬃcient estimation (sparse greedy)\n",
      "5.2.3 Heteroscedastic Estimation\n",
      "explain that we have two parameters. not too many details (do that as an\n",
      "\n",
      "when attempting to classify webpages, y might be a permutation, when\n",
      "attempting to match objects, to perform collaborative ﬁltering, or to rank\n",
      "documents in a retrieval setting. Equally well, y might be an annotation of\n",
      "a text, when performing named entity recognition. Each of those problems\n",
      "has its own properties in terms of the set of y which we might consider\n",
      "admissible, or how to search this space. We will discuss a number of those\n",
      "problems in Chapter ??.\n",
      "Regression is another prototypical application. Here the goal is to esti-\n",
      "mate a real-valued variable y∈R given a pattern x(see e.g. Figure 1.7). For\n",
      "instance, we might want to estimate the value of a stock the next day, the\n",
      "yield of a semiconductor fab given the current process, the iron content of\n",
      "ore given mass spectroscopy measurements, or the heart rate of an athlete,\n",
      "given accelerometer data. One of the key issues in which regression problems\n",
      "diﬀer from each other is the choice of a loss. For instance, when estimating\n",
      "\n",
      "1.1 A Taste of Machine Learning 11\n",
      "Fig. 1.7. Regression estimation. We are given a number of instances (indicated by\n",
      "black dots) and would like to ﬁnd some function f mapping the observations X to\n",
      "R such that f(x) is close to the observed values.\n",
      "error we make. For instance, in the problem of assessing the risk of cancer, it\n",
      "makes a signiﬁcant diﬀerence whether we mis-classify an early stage of can-\n",
      "cer as healthy (in which case the patient is likely to die) or as an advanced\n",
      "stage of cancer (in which case the patient is likely to be inconvenienced from\n",
      "overly aggressive treatment).\n",
      "Structured Estimation goes beyond simple multiclass estimation by\n",
      "assuming that the labels yhave some additional structure which can be used\n",
      "in the estimation process. For instance, y might be a path in an ontology,\n",
      "when attempting to classify webpages, y might be a permutation, when\n",
      "attempting to match objects, to perform collaborative ﬁltering, or to rank\n",
      "\n",
      "184 7 Linear Models\n",
      "allow us to draw many useful conclusions:\n",
      "•Whenever |yi −(⟨w,xi⟩+ b)|< ϵ, this implies that ξ+\n",
      "i = ξ−\n",
      "i = α+\n",
      "i =\n",
      "α−\n",
      "i = 0. In other words, points which lie inside the ϵ tube around the\n",
      "hyperplane ⟨w,x⟩+ b do not contribute to the solution thus leading to\n",
      "sparse expansions in terms of α.\n",
      "•If (⟨w,xi⟩+b)−yi >ϵ we haveξ−\n",
      "i >0 and therefore α−\n",
      "i = C\n",
      "m. On the other\n",
      "hand, ξ+ = 0 and α+\n",
      "i = 0. The case yi −(⟨w,xi⟩+ b) > ϵis symmetric\n",
      "and yields ξ−= 0, ξ+\n",
      "i >0, α+\n",
      "i = C\n",
      "m, and α−\n",
      "i = 0.\n",
      "•Finally, if (⟨w,xi⟩+ b) −yi = ϵ we have ξ−\n",
      "i = 0 and 0 ≤α−\n",
      "i ≤C\n",
      "m, while\n",
      "ξ+ = 0 and α+\n",
      "i = 0. Similarly, when yi −(⟨w,xi⟩+ b) = ϵ we obtain\n",
      "ξ+\n",
      "i = 0, 0 ≤α+\n",
      "i ≤C\n",
      "m, ξ−= 0 and α−\n",
      "i = 0.\n",
      "Note that α+\n",
      "i and α−\n",
      "i are never simultaneously non-zero.\n",
      "7.3.1 Incorporating General Loss Functions\n",
      "Using the same reasoning as in Section 7.1.1 we can deduce from (7.52) that\n",
      "the loss function of support vector regression is given by\n",
      "l(w,x,y ) = max(0,|y−⟨w,x⟩|− ϵ). (7.59)\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Based on the following context, create 5 flashcards about linear regression.\n",
      "Each flashcard should have a question on one side and the answer on the other.\n",
      "Format as:\n",
      "\n",
      "Q: [Question]\n",
      "A: [Answer]\n",
      "\n",
      "Context: 5.2 Regression 151\n",
      "To minimize the above objective function we ﬁrst compute the gradient.\n",
      "∇J(θ) = θ+\n",
      "m∑\n",
      "i=1\n",
      "exp\n",
      "(⣨\n",
      "−yiˆφ(xi),θ\n",
      "⟩)\n",
      "1 + exp\n",
      "(⣨\n",
      "−yiˆφ(xi),θ\n",
      "⟩)(−yiˆφ(xi))\n",
      "= θ+\n",
      "m∑\n",
      "i=1\n",
      "(p(yi|xi,θ) −1)yiˆφ(xi).\n",
      "Notice that the second term of the gradient vanishes whenever p(yi|xi,θ) =\n",
      "1. Therefore, one way to interpret logistic regression is to view it as a method\n",
      "to maximize p(yi|xi,θ) for each point ( xi,yi) in the training set. Since the\n",
      "objective function of logistic regression is twice diﬀerentiable one can also\n",
      "compute its Hessian\n",
      "∇2J(θ) = I−\n",
      "m∑\n",
      "i=1\n",
      "p(yi|xi,θ)(1 −p(yi|xi,θ))ˆφ(xi)ˆφ(xi)⊤,\n",
      "where we used y2\n",
      "i = 1. The Hessian can be used in the Newton method\n",
      "(Section 3.2.6) to obtain the optimal parameter θ.\n",
      "5.2 Regression\n",
      "5.2.1 Conditionally Normal Models\n",
      "ﬁxed variance\n",
      "5.2.2 Posterior Distribution\n",
      "integrating out vs. Laplace approximation, eﬃcient estimation (sparse greedy)\n",
      "5.2.3 Heteroscedastic Estimation\n",
      "explain that we have two parameters. not too many details (do that as an\n",
      "\n",
      "when attempting to classify webpages, y might be a permutation, when\n",
      "attempting to match objects, to perform collaborative ﬁltering, or to rank\n",
      "documents in a retrieval setting. Equally well, y might be an annotation of\n",
      "a text, when performing named entity recognition. Each of those problems\n",
      "has its own properties in terms of the set of y which we might consider\n",
      "admissible, or how to search this space. We will discuss a number of those\n",
      "problems in Chapter ??.\n",
      "Regression is another prototypical application. Here the goal is to esti-\n",
      "mate a real-valued variable y∈R given a pattern x(see e.g. Figure 1.7). For\n",
      "instance, we might want to estimate the value of a stock the next day, the\n",
      "yield of a semiconductor fab given the current process, the iron content of\n",
      "ore given mass spectroscopy measurements, or the heart rate of an athlete,\n",
      "given accelerometer data. One of the key issues in which regression problems\n",
      "diﬀer from each other is the choice of a loss. For instance, when estimating\n",
      "\n",
      "1.1 A Taste of Machine Learning 11\n",
      "Fig. 1.7. Regression estimation. We are given a number of instances (indicated by\n",
      "black dots) and would like to ﬁnd some function f mapping the observations X to\n",
      "R such that f(x) is close to the observed values.\n",
      "error we make. For instance, in the problem of assessing the risk of cancer, it\n",
      "makes a signiﬁcant diﬀerence whether we mis-classify an early stage of can-\n",
      "cer as healthy (in which case the patient is likely to die) or as an advanced\n",
      "stage of cancer (in which case the patient is likely to be inconvenienced from\n",
      "overly aggressive treatment).\n",
      "Structured Estimation goes beyond simple multiclass estimation by\n",
      "assuming that the labels yhave some additional structure which can be used\n",
      "in the estimation process. For instance, y might be a path in an ontology,\n",
      "when attempting to classify webpages, y might be a permutation, when\n",
      "attempting to match objects, to perform collaborative ﬁltering, or to rank\n",
      "\n",
      "184 7 Linear Models\n",
      "allow us to draw many useful conclusions:\n",
      "•Whenever |yi −(⟨w,xi⟩+ b)|< ϵ, this implies that ξ+\n",
      "i = ξ−\n",
      "i = α+\n",
      "i =\n",
      "α−\n",
      "i = 0. In other words, points which lie inside the ϵ tube around the\n",
      "hyperplane ⟨w,x⟩+ b do not contribute to the solution thus leading to\n",
      "sparse expansions in terms of α.\n",
      "•If (⟨w,xi⟩+b)−yi >ϵ we haveξ−\n",
      "i >0 and therefore α−\n",
      "i = C\n",
      "m. On the other\n",
      "hand, ξ+ = 0 and α+\n",
      "i = 0. The case yi −(⟨w,xi⟩+ b) > ϵis symmetric\n",
      "and yields ξ−= 0, ξ+\n",
      "i >0, α+\n",
      "i = C\n",
      "m, and α−\n",
      "i = 0.\n",
      "•Finally, if (⟨w,xi⟩+ b) −yi = ϵ we have ξ−\n",
      "i = 0 and 0 ≤α−\n",
      "i ≤C\n",
      "m, while\n",
      "ξ+ = 0 and α+\n",
      "i = 0. Similarly, when yi −(⟨w,xi⟩+ b) = ϵ we obtain\n",
      "ξ+\n",
      "i = 0, 0 ≤α+\n",
      "i ≤C\n",
      "m, ξ−= 0 and α−\n",
      "i = 0.\n",
      "Note that α+\n",
      "i and α−\n",
      "i are never simultaneously non-zero.\n",
      "7.3.1 Incorporating General Loss Functions\n",
      "Using the same reasoning as in Section 7.1.1 we can deduce from (7.52) that\n",
      "the loss function of support vector regression is given by\n",
      "l(w,x,y ) = max(0,|y−⟨w,x⟩|− ϵ). (7.59)\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Based on the following context, create 2 practice problems about linear regression.\n",
      "These should be application-based problems that test understanding rather than recall.\n",
      "Provide detailed solutions to each problem.\n",
      "\n",
      "Context: 5.2 Regression 151\n",
      "To minimize the above objective function we ﬁrst compute the gradient.\n",
      "∇J(θ) = θ+\n",
      "m∑\n",
      "i=1\n",
      "exp\n",
      "(⣨\n",
      "−yiˆφ(xi),θ\n",
      "⟩)\n",
      "1 + exp\n",
      "(⣨\n",
      "−yiˆφ(xi),θ\n",
      "⟩)(−yiˆφ(xi))\n",
      "= θ+\n",
      "m∑\n",
      "i=1\n",
      "(p(yi|xi,θ) −1)yiˆφ(xi).\n",
      "Notice that the second term of the gradient vanishes whenever p(yi|xi,θ) =\n",
      "1. Therefore, one way to interpret logistic regression is to view it as a method\n",
      "to maximize p(yi|xi,θ) for each point ( xi,yi) in the training set. Since the\n",
      "objective function of logistic regression is twice diﬀerentiable one can also\n",
      "compute its Hessian\n",
      "∇2J(θ) = I−\n",
      "m∑\n",
      "i=1\n",
      "p(yi|xi,θ)(1 −p(yi|xi,θ))ˆφ(xi)ˆφ(xi)⊤,\n",
      "where we used y2\n",
      "i = 1. The Hessian can be used in the Newton method\n",
      "(Section 3.2.6) to obtain the optimal parameter θ.\n",
      "5.2 Regression\n",
      "5.2.1 Conditionally Normal Models\n",
      "ﬁxed variance\n",
      "5.2.2 Posterior Distribution\n",
      "integrating out vs. Laplace approximation, eﬃcient estimation (sparse greedy)\n",
      "5.2.3 Heteroscedastic Estimation\n",
      "explain that we have two parameters. not too many details (do that as an\n",
      "\n",
      "when attempting to classify webpages, y might be a permutation, when\n",
      "attempting to match objects, to perform collaborative ﬁltering, or to rank\n",
      "documents in a retrieval setting. Equally well, y might be an annotation of\n",
      "a text, when performing named entity recognition. Each of those problems\n",
      "has its own properties in terms of the set of y which we might consider\n",
      "admissible, or how to search this space. We will discuss a number of those\n",
      "problems in Chapter ??.\n",
      "Regression is another prototypical application. Here the goal is to esti-\n",
      "mate a real-valued variable y∈R given a pattern x(see e.g. Figure 1.7). For\n",
      "instance, we might want to estimate the value of a stock the next day, the\n",
      "yield of a semiconductor fab given the current process, the iron content of\n",
      "ore given mass spectroscopy measurements, or the heart rate of an athlete,\n",
      "given accelerometer data. One of the key issues in which regression problems\n",
      "diﬀer from each other is the choice of a loss. For instance, when estimating\n",
      "\n",
      "1.1 A Taste of Machine Learning 11\n",
      "Fig. 1.7. Regression estimation. We are given a number of instances (indicated by\n",
      "black dots) and would like to ﬁnd some function f mapping the observations X to\n",
      "R such that f(x) is close to the observed values.\n",
      "error we make. For instance, in the problem of assessing the risk of cancer, it\n",
      "makes a signiﬁcant diﬀerence whether we mis-classify an early stage of can-\n",
      "cer as healthy (in which case the patient is likely to die) or as an advanced\n",
      "stage of cancer (in which case the patient is likely to be inconvenienced from\n",
      "overly aggressive treatment).\n",
      "Structured Estimation goes beyond simple multiclass estimation by\n",
      "assuming that the labels yhave some additional structure which can be used\n",
      "in the estimation process. For instance, y might be a path in an ontology,\n",
      "when attempting to classify webpages, y might be a permutation, when\n",
      "attempting to match objects, to perform collaborative ﬁltering, or to rank\n",
      "\n",
      "184 7 Linear Models\n",
      "allow us to draw many useful conclusions:\n",
      "•Whenever |yi −(⟨w,xi⟩+ b)|< ϵ, this implies that ξ+\n",
      "i = ξ−\n",
      "i = α+\n",
      "i =\n",
      "α−\n",
      "i = 0. In other words, points which lie inside the ϵ tube around the\n",
      "hyperplane ⟨w,x⟩+ b do not contribute to the solution thus leading to\n",
      "sparse expansions in terms of α.\n",
      "•If (⟨w,xi⟩+b)−yi >ϵ we haveξ−\n",
      "i >0 and therefore α−\n",
      "i = C\n",
      "m. On the other\n",
      "hand, ξ+ = 0 and α+\n",
      "i = 0. The case yi −(⟨w,xi⟩+ b) > ϵis symmetric\n",
      "and yields ξ−= 0, ξ+\n",
      "i >0, α+\n",
      "i = C\n",
      "m, and α−\n",
      "i = 0.\n",
      "•Finally, if (⟨w,xi⟩+ b) −yi = ϵ we have ξ−\n",
      "i = 0 and 0 ≤α−\n",
      "i ≤C\n",
      "m, while\n",
      "ξ+ = 0 and α+\n",
      "i = 0. Similarly, when yi −(⟨w,xi⟩+ b) = ϵ we obtain\n",
      "ξ+\n",
      "i = 0, 0 ≤α+\n",
      "i ≤C\n",
      "m, ξ−= 0 and α−\n",
      "i = 0.\n",
      "Note that α+\n",
      "i and α−\n",
      "i are never simultaneously non-zero.\n",
      "7.3.1 Incorporating General Loss Functions\n",
      "Using the same reasoning as in Section 7.1.1 we can deduce from (7.52) that\n",
      "the loss function of support vector regression is given by\n",
      "l(w,x,y ) = max(0,|y−⟨w,x⟩|− ϵ). (7.59)\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Based on the following context, create a quiz with 3 multiple-choice questions about svm. \n",
      "For each question, provide 4 options and indicate the correct answer.\n",
      "\n",
      "Context: As before, we can replace (7.21) by a linear penalty for constraint viola-\n",
      "tion in order to recover (7.5). The quantity log p(yi|xi,θ)\n",
      "maxy̸=yip(y|xi,θ) is sometimes\n",
      "called the log-odds ratio, and the above discussion shows that SVMs can\n",
      "be interpreted as maximizing the log-odds ratio in the exponential family.\n",
      "This interpretation will be developed further when we consider extensions of\n",
      "SVMs to tackle multiclass, multilabel, and structured prediction problems.\n",
      "7.1.3 Specialized Algorithms for Training SVMs\n",
      "The main task in training SVMs boils down to solving (7.9). The m×m\n",
      "matrix H is usually dense and cannot be stored in memory. Decomposition\n",
      "methods are designed to overcome these diﬃculties. The basic idea here\n",
      "is to identify and update a small working set B by solving a small sub-\n",
      "problem at every iteration. Formally, let B ⊂{1,...,m }be the working set\n",
      "and αB be the corresponding sub-vector of α. Deﬁne ¯B = {1,...,m }\\ B\n",
      "\n",
      "Berlin, 1982.\n",
      "[Vap95] , The nature of statistical learning theory, Springer, New York, 1995.\n",
      "[Vap98] , Statistical learning theory, John Wiley and Sons, New York, 1998.\n",
      "[vdG00] S. van de Geer, Empirical processes in M-estimation, Cambridge University\n",
      "Press, 2000.\n",
      "[vdVW96] A. W. van der Vaart and J. A. Wellner, Weak convergence and empirical\n",
      "processes, Springer, 1996.\n",
      "[VGS97] V. Vapnik, S. Golowich, and A. J. Smola, Support vector method for func-\n",
      "tion approximation, regression estimation, and signal processing , Advances in\n",
      "Neural Information Processing Systems 9 (Cambridge, MA) (M. C. Mozer,\n",
      "M. I. Jordan, and T. Petsche, eds.), MIT Press, 1997, pp. 281–287.\n",
      "[Voo01] E. Voorhees, Overview of the TRECT 2001 question answering track ,\n",
      "TREC, 2001.\n",
      "[VS04] S. V. N. Vishwanathan and A. J. Smola, Fast kernels for string and\n",
      "tree matching, Kernel Methods in Computational Biology (Cambridge, MA)\n",
      "(B. Sch¨ olkopf, K. Tsuda, and J. P. Vert, eds.), MIT Press, 2004, pp. 113–130.\n",
      "\n",
      "tree matching, Kernel Methods in Computational Biology (Cambridge, MA)\n",
      "(B. Sch¨ olkopf, K. Tsuda, and J. P. Vert, eds.), MIT Press, 2004, pp. 113–130.\n",
      "[VSV07] S. V. N. Vishwanathan, A. J. Smola, and R. Vidal, Binet-Cauchy kernels\n",
      "on dynamical systems and its application to the analysis of dynamic scenes ,\n",
      "International Journal of Computer Vision 73 (2007), no. 1, 95–119.\n",
      "[Wah97] G. Wahba, Support vector machines, reproducing kernel Hilbert spaces and\n",
      "the randomized GACV, Tech. Report 984, Department of Statistics, University\n",
      "of Wisconsin, Madison, 1997.\n",
      "[Wat64] G. S. Watson, Smooth regression analysis, Sankhya A 26 (1964), 359–372.\n",
      "[Wil98] C. K. I. Williams, Prediction with Gaussian processes: From linear regression\n",
      "to linear prediction and beyond , Learning and Inference in Graphical Models\n",
      "(M. I. Jordan, ed.), Kluwer Academic, 1998, pp. 599–621.\n",
      "\n",
      "Introduction to Machine Learning\n",
      "Alex Smola and S.V.N. Vishwanathan\n",
      "Yahoo! Labs\n",
      "Santa Clara\n",
      "–and–\n",
      "Departments of Statistics and Computer Science\n",
      "Purdue University\n",
      "–and–\n",
      "College of Engineering and Computer Science\n",
      "Australian National University\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Based on the following context, create a quiz with 3 multiple-choice questions about svm. \n",
      "For each question, provide 4 options and indicate the correct answer.\n",
      "\n",
      "Context: As before, we can replace (7.21) by a linear penalty for constraint viola-\n",
      "tion in order to recover (7.5). The quantity log p(yi|xi,θ)\n",
      "maxy̸=yip(y|xi,θ) is sometimes\n",
      "called the log-odds ratio, and the above discussion shows that SVMs can\n",
      "be interpreted as maximizing the log-odds ratio in the exponential family.\n",
      "This interpretation will be developed further when we consider extensions of\n",
      "SVMs to tackle multiclass, multilabel, and structured prediction problems.\n",
      "7.1.3 Specialized Algorithms for Training SVMs\n",
      "The main task in training SVMs boils down to solving (7.9). The m×m\n",
      "matrix H is usually dense and cannot be stored in memory. Decomposition\n",
      "methods are designed to overcome these diﬃculties. The basic idea here\n",
      "is to identify and update a small working set B by solving a small sub-\n",
      "problem at every iteration. Formally, let B ⊂{1,...,m }be the working set\n",
      "and αB be the corresponding sub-vector of α. Deﬁne ¯B = {1,...,m }\\ B\n",
      "\n",
      "Berlin, 1982.\n",
      "[Vap95] , The nature of statistical learning theory, Springer, New York, 1995.\n",
      "[Vap98] , Statistical learning theory, John Wiley and Sons, New York, 1998.\n",
      "[vdG00] S. van de Geer, Empirical processes in M-estimation, Cambridge University\n",
      "Press, 2000.\n",
      "[vdVW96] A. W. van der Vaart and J. A. Wellner, Weak convergence and empirical\n",
      "processes, Springer, 1996.\n",
      "[VGS97] V. Vapnik, S. Golowich, and A. J. Smola, Support vector method for func-\n",
      "tion approximation, regression estimation, and signal processing , Advances in\n",
      "Neural Information Processing Systems 9 (Cambridge, MA) (M. C. Mozer,\n",
      "M. I. Jordan, and T. Petsche, eds.), MIT Press, 1997, pp. 281–287.\n",
      "[Voo01] E. Voorhees, Overview of the TRECT 2001 question answering track ,\n",
      "TREC, 2001.\n",
      "[VS04] S. V. N. Vishwanathan and A. J. Smola, Fast kernels for string and\n",
      "tree matching, Kernel Methods in Computational Biology (Cambridge, MA)\n",
      "(B. Sch¨ olkopf, K. Tsuda, and J. P. Vert, eds.), MIT Press, 2004, pp. 113–130.\n",
      "\n",
      "tree matching, Kernel Methods in Computational Biology (Cambridge, MA)\n",
      "(B. Sch¨ olkopf, K. Tsuda, and J. P. Vert, eds.), MIT Press, 2004, pp. 113–130.\n",
      "[VSV07] S. V. N. Vishwanathan, A. J. Smola, and R. Vidal, Binet-Cauchy kernels\n",
      "on dynamical systems and its application to the analysis of dynamic scenes ,\n",
      "International Journal of Computer Vision 73 (2007), no. 1, 95–119.\n",
      "[Wah97] G. Wahba, Support vector machines, reproducing kernel Hilbert spaces and\n",
      "the randomized GACV, Tech. Report 984, Department of Statistics, University\n",
      "of Wisconsin, Madison, 1997.\n",
      "[Wat64] G. S. Watson, Smooth regression analysis, Sankhya A 26 (1964), 359–372.\n",
      "[Wil98] C. K. I. Williams, Prediction with Gaussian processes: From linear regression\n",
      "to linear prediction and beyond , Learning and Inference in Graphical Models\n",
      "(M. I. Jordan, ed.), Kluwer Academic, 1998, pp. 599–621.\n",
      "\n",
      "Introduction to Machine Learning\n",
      "Alex Smola and S.V.N. Vishwanathan\n",
      "Yahoo! Labs\n",
      "Santa Clara\n",
      "–and–\n",
      "Departments of Statistics and Computer Science\n",
      "Purdue University\n",
      "–and–\n",
      "College of Engineering and Computer Science\n",
      "Australian National University\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Based on the following context, create 2 practice problems about linear regression.\n",
      "These should be application-based problems that test understanding rather than recall.\n",
      "Provide detailed solutions to each problem.\n",
      "\n",
      "Context: 5.2 Regression 151\n",
      "To minimize the above objective function we ﬁrst compute the gradient.\n",
      "∇J(θ) = θ+\n",
      "m∑\n",
      "i=1\n",
      "exp\n",
      "(⣨\n",
      "−yiˆφ(xi),θ\n",
      "⟩)\n",
      "1 + exp\n",
      "(⣨\n",
      "−yiˆφ(xi),θ\n",
      "⟩)(−yiˆφ(xi))\n",
      "= θ+\n",
      "m∑\n",
      "i=1\n",
      "(p(yi|xi,θ) −1)yiˆφ(xi).\n",
      "Notice that the second term of the gradient vanishes whenever p(yi|xi,θ) =\n",
      "1. Therefore, one way to interpret logistic regression is to view it as a method\n",
      "to maximize p(yi|xi,θ) for each point ( xi,yi) in the training set. Since the\n",
      "objective function of logistic regression is twice diﬀerentiable one can also\n",
      "compute its Hessian\n",
      "∇2J(θ) = I−\n",
      "m∑\n",
      "i=1\n",
      "p(yi|xi,θ)(1 −p(yi|xi,θ))ˆφ(xi)ˆφ(xi)⊤,\n",
      "where we used y2\n",
      "i = 1. The Hessian can be used in the Newton method\n",
      "(Section 3.2.6) to obtain the optimal parameter θ.\n",
      "5.2 Regression\n",
      "5.2.1 Conditionally Normal Models\n",
      "ﬁxed variance\n",
      "5.2.2 Posterior Distribution\n",
      "integrating out vs. Laplace approximation, eﬃcient estimation (sparse greedy)\n",
      "5.2.3 Heteroscedastic Estimation\n",
      "explain that we have two parameters. not too many details (do that as an\n",
      "\n",
      "when attempting to classify webpages, y might be a permutation, when\n",
      "attempting to match objects, to perform collaborative ﬁltering, or to rank\n",
      "documents in a retrieval setting. Equally well, y might be an annotation of\n",
      "a text, when performing named entity recognition. Each of those problems\n",
      "has its own properties in terms of the set of y which we might consider\n",
      "admissible, or how to search this space. We will discuss a number of those\n",
      "problems in Chapter ??.\n",
      "Regression is another prototypical application. Here the goal is to esti-\n",
      "mate a real-valued variable y∈R given a pattern x(see e.g. Figure 1.7). For\n",
      "instance, we might want to estimate the value of a stock the next day, the\n",
      "yield of a semiconductor fab given the current process, the iron content of\n",
      "ore given mass spectroscopy measurements, or the heart rate of an athlete,\n",
      "given accelerometer data. One of the key issues in which regression problems\n",
      "diﬀer from each other is the choice of a loss. For instance, when estimating\n",
      "\n",
      "1.1 A Taste of Machine Learning 11\n",
      "Fig. 1.7. Regression estimation. We are given a number of instances (indicated by\n",
      "black dots) and would like to ﬁnd some function f mapping the observations X to\n",
      "R such that f(x) is close to the observed values.\n",
      "error we make. For instance, in the problem of assessing the risk of cancer, it\n",
      "makes a signiﬁcant diﬀerence whether we mis-classify an early stage of can-\n",
      "cer as healthy (in which case the patient is likely to die) or as an advanced\n",
      "stage of cancer (in which case the patient is likely to be inconvenienced from\n",
      "overly aggressive treatment).\n",
      "Structured Estimation goes beyond simple multiclass estimation by\n",
      "assuming that the labels yhave some additional structure which can be used\n",
      "in the estimation process. For instance, y might be a path in an ontology,\n",
      "when attempting to classify webpages, y might be a permutation, when\n",
      "attempting to match objects, to perform collaborative ﬁltering, or to rank\n",
      "\n",
      "184 7 Linear Models\n",
      "allow us to draw many useful conclusions:\n",
      "•Whenever |yi −(⟨w,xi⟩+ b)|< ϵ, this implies that ξ+\n",
      "i = ξ−\n",
      "i = α+\n",
      "i =\n",
      "α−\n",
      "i = 0. In other words, points which lie inside the ϵ tube around the\n",
      "hyperplane ⟨w,x⟩+ b do not contribute to the solution thus leading to\n",
      "sparse expansions in terms of α.\n",
      "•If (⟨w,xi⟩+b)−yi >ϵ we haveξ−\n",
      "i >0 and therefore α−\n",
      "i = C\n",
      "m. On the other\n",
      "hand, ξ+ = 0 and α+\n",
      "i = 0. The case yi −(⟨w,xi⟩+ b) > ϵis symmetric\n",
      "and yields ξ−= 0, ξ+\n",
      "i >0, α+\n",
      "i = C\n",
      "m, and α−\n",
      "i = 0.\n",
      "•Finally, if (⟨w,xi⟩+ b) −yi = ϵ we have ξ−\n",
      "i = 0 and 0 ≤α−\n",
      "i ≤C\n",
      "m, while\n",
      "ξ+ = 0 and α+\n",
      "i = 0. Similarly, when yi −(⟨w,xi⟩+ b) = ϵ we obtain\n",
      "ξ+\n",
      "i = 0, 0 ≤α+\n",
      "i ≤C\n",
      "m, ξ−= 0 and α−\n",
      "i = 0.\n",
      "Note that α+\n",
      "i and α−\n",
      "i are never simultaneously non-zero.\n",
      "7.3.1 Incorporating General Loss Functions\n",
      "Using the same reasoning as in Section 7.1.1 we can deduce from (7.52) that\n",
      "the loss function of support vector regression is given by\n",
      "l(w,x,y ) = max(0,|y−⟨w,x⟩|− ϵ). (7.59)\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Based on the following context, create 5 flashcards about linear regression.\n",
      "Each flashcard should have a question on one side and the answer on the other.\n",
      "Format as:\n",
      "\n",
      "Q: [Question]\n",
      "A: [Answer]\n",
      "\n",
      "Context: 5.2 Regression 151\n",
      "To minimize the above objective function we ﬁrst compute the gradient.\n",
      "∇J(θ) = θ+\n",
      "m∑\n",
      "i=1\n",
      "exp\n",
      "(⣨\n",
      "−yiˆφ(xi),θ\n",
      "⟩)\n",
      "1 + exp\n",
      "(⣨\n",
      "−yiˆφ(xi),θ\n",
      "⟩)(−yiˆφ(xi))\n",
      "= θ+\n",
      "m∑\n",
      "i=1\n",
      "(p(yi|xi,θ) −1)yiˆφ(xi).\n",
      "Notice that the second term of the gradient vanishes whenever p(yi|xi,θ) =\n",
      "1. Therefore, one way to interpret logistic regression is to view it as a method\n",
      "to maximize p(yi|xi,θ) for each point ( xi,yi) in the training set. Since the\n",
      "objective function of logistic regression is twice diﬀerentiable one can also\n",
      "compute its Hessian\n",
      "∇2J(θ) = I−\n",
      "m∑\n",
      "i=1\n",
      "p(yi|xi,θ)(1 −p(yi|xi,θ))ˆφ(xi)ˆφ(xi)⊤,\n",
      "where we used y2\n",
      "i = 1. The Hessian can be used in the Newton method\n",
      "(Section 3.2.6) to obtain the optimal parameter θ.\n",
      "5.2 Regression\n",
      "5.2.1 Conditionally Normal Models\n",
      "ﬁxed variance\n",
      "5.2.2 Posterior Distribution\n",
      "integrating out vs. Laplace approximation, eﬃcient estimation (sparse greedy)\n",
      "5.2.3 Heteroscedastic Estimation\n",
      "explain that we have two parameters. not too many details (do that as an\n",
      "\n",
      "when attempting to classify webpages, y might be a permutation, when\n",
      "attempting to match objects, to perform collaborative ﬁltering, or to rank\n",
      "documents in a retrieval setting. Equally well, y might be an annotation of\n",
      "a text, when performing named entity recognition. Each of those problems\n",
      "has its own properties in terms of the set of y which we might consider\n",
      "admissible, or how to search this space. We will discuss a number of those\n",
      "problems in Chapter ??.\n",
      "Regression is another prototypical application. Here the goal is to esti-\n",
      "mate a real-valued variable y∈R given a pattern x(see e.g. Figure 1.7). For\n",
      "instance, we might want to estimate the value of a stock the next day, the\n",
      "yield of a semiconductor fab given the current process, the iron content of\n",
      "ore given mass spectroscopy measurements, or the heart rate of an athlete,\n",
      "given accelerometer data. One of the key issues in which regression problems\n",
      "diﬀer from each other is the choice of a loss. For instance, when estimating\n",
      "\n",
      "1.1 A Taste of Machine Learning 11\n",
      "Fig. 1.7. Regression estimation. We are given a number of instances (indicated by\n",
      "black dots) and would like to ﬁnd some function f mapping the observations X to\n",
      "R such that f(x) is close to the observed values.\n",
      "error we make. For instance, in the problem of assessing the risk of cancer, it\n",
      "makes a signiﬁcant diﬀerence whether we mis-classify an early stage of can-\n",
      "cer as healthy (in which case the patient is likely to die) or as an advanced\n",
      "stage of cancer (in which case the patient is likely to be inconvenienced from\n",
      "overly aggressive treatment).\n",
      "Structured Estimation goes beyond simple multiclass estimation by\n",
      "assuming that the labels yhave some additional structure which can be used\n",
      "in the estimation process. For instance, y might be a path in an ontology,\n",
      "when attempting to classify webpages, y might be a permutation, when\n",
      "attempting to match objects, to perform collaborative ﬁltering, or to rank\n",
      "\n",
      "184 7 Linear Models\n",
      "allow us to draw many useful conclusions:\n",
      "•Whenever |yi −(⟨w,xi⟩+ b)|< ϵ, this implies that ξ+\n",
      "i = ξ−\n",
      "i = α+\n",
      "i =\n",
      "α−\n",
      "i = 0. In other words, points which lie inside the ϵ tube around the\n",
      "hyperplane ⟨w,x⟩+ b do not contribute to the solution thus leading to\n",
      "sparse expansions in terms of α.\n",
      "•If (⟨w,xi⟩+b)−yi >ϵ we haveξ−\n",
      "i >0 and therefore α−\n",
      "i = C\n",
      "m. On the other\n",
      "hand, ξ+ = 0 and α+\n",
      "i = 0. The case yi −(⟨w,xi⟩+ b) > ϵis symmetric\n",
      "and yields ξ−= 0, ξ+\n",
      "i >0, α+\n",
      "i = C\n",
      "m, and α−\n",
      "i = 0.\n",
      "•Finally, if (⟨w,xi⟩+ b) −yi = ϵ we have ξ−\n",
      "i = 0 and 0 ≤α−\n",
      "i ≤C\n",
      "m, while\n",
      "ξ+ = 0 and α+\n",
      "i = 0. Similarly, when yi −(⟨w,xi⟩+ b) = ϵ we obtain\n",
      "ξ+\n",
      "i = 0, 0 ≤α+\n",
      "i ≤C\n",
      "m, ξ−= 0 and α−\n",
      "i = 0.\n",
      "Note that α+\n",
      "i and α−\n",
      "i are never simultaneously non-zero.\n",
      "7.3.1 Incorporating General Loss Functions\n",
      "Using the same reasoning as in Section 7.1.1 we can deduce from (7.52) that\n",
      "the loss function of support vector regression is given by\n",
      "l(w,x,y ) = max(0,|y−⟨w,x⟩|− ϵ). (7.59)\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Based on the following context, create a concise study guide about linear regression.\n",
      "Include key definitions, main concepts, and important relationships.\n",
      "Format it with clear sections and bullet points for readability.\n",
      "\n",
      "Context: 5.2 Regression 151\n",
      "To minimize the above objective function we ﬁrst compute the gradient.\n",
      "∇J(θ) = θ+\n",
      "m∑\n",
      "i=1\n",
      "exp\n",
      "(⣨\n",
      "−yiˆφ(xi),θ\n",
      "⟩)\n",
      "1 + exp\n",
      "(⣨\n",
      "−yiˆφ(xi),θ\n",
      "⟩)(−yiˆφ(xi))\n",
      "= θ+\n",
      "m∑\n",
      "i=1\n",
      "(p(yi|xi,θ) −1)yiˆφ(xi).\n",
      "Notice that the second term of the gradient vanishes whenever p(yi|xi,θ) =\n",
      "1. Therefore, one way to interpret logistic regression is to view it as a method\n",
      "to maximize p(yi|xi,θ) for each point ( xi,yi) in the training set. Since the\n",
      "objective function of logistic regression is twice diﬀerentiable one can also\n",
      "compute its Hessian\n",
      "∇2J(θ) = I−\n",
      "m∑\n",
      "i=1\n",
      "p(yi|xi,θ)(1 −p(yi|xi,θ))ˆφ(xi)ˆφ(xi)⊤,\n",
      "where we used y2\n",
      "i = 1. The Hessian can be used in the Newton method\n",
      "(Section 3.2.6) to obtain the optimal parameter θ.\n",
      "5.2 Regression\n",
      "5.2.1 Conditionally Normal Models\n",
      "ﬁxed variance\n",
      "5.2.2 Posterior Distribution\n",
      "integrating out vs. Laplace approximation, eﬃcient estimation (sparse greedy)\n",
      "5.2.3 Heteroscedastic Estimation\n",
      "explain that we have two parameters. not too many details (do that as an\n",
      "\n",
      "when attempting to classify webpages, y might be a permutation, when\n",
      "attempting to match objects, to perform collaborative ﬁltering, or to rank\n",
      "documents in a retrieval setting. Equally well, y might be an annotation of\n",
      "a text, when performing named entity recognition. Each of those problems\n",
      "has its own properties in terms of the set of y which we might consider\n",
      "admissible, or how to search this space. We will discuss a number of those\n",
      "problems in Chapter ??.\n",
      "Regression is another prototypical application. Here the goal is to esti-\n",
      "mate a real-valued variable y∈R given a pattern x(see e.g. Figure 1.7). For\n",
      "instance, we might want to estimate the value of a stock the next day, the\n",
      "yield of a semiconductor fab given the current process, the iron content of\n",
      "ore given mass spectroscopy measurements, or the heart rate of an athlete,\n",
      "given accelerometer data. One of the key issues in which regression problems\n",
      "diﬀer from each other is the choice of a loss. For instance, when estimating\n",
      "\n",
      "1.1 A Taste of Machine Learning 11\n",
      "Fig. 1.7. Regression estimation. We are given a number of instances (indicated by\n",
      "black dots) and would like to ﬁnd some function f mapping the observations X to\n",
      "R such that f(x) is close to the observed values.\n",
      "error we make. For instance, in the problem of assessing the risk of cancer, it\n",
      "makes a signiﬁcant diﬀerence whether we mis-classify an early stage of can-\n",
      "cer as healthy (in which case the patient is likely to die) or as an advanced\n",
      "stage of cancer (in which case the patient is likely to be inconvenienced from\n",
      "overly aggressive treatment).\n",
      "Structured Estimation goes beyond simple multiclass estimation by\n",
      "assuming that the labels yhave some additional structure which can be used\n",
      "in the estimation process. For instance, y might be a path in an ontology,\n",
      "when attempting to classify webpages, y might be a permutation, when\n",
      "attempting to match objects, to perform collaborative ﬁltering, or to rank\n",
      "\n",
      "184 7 Linear Models\n",
      "allow us to draw many useful conclusions:\n",
      "•Whenever |yi −(⟨w,xi⟩+ b)|< ϵ, this implies that ξ+\n",
      "i = ξ−\n",
      "i = α+\n",
      "i =\n",
      "α−\n",
      "i = 0. In other words, points which lie inside the ϵ tube around the\n",
      "hyperplane ⟨w,x⟩+ b do not contribute to the solution thus leading to\n",
      "sparse expansions in terms of α.\n",
      "•If (⟨w,xi⟩+b)−yi >ϵ we haveξ−\n",
      "i >0 and therefore α−\n",
      "i = C\n",
      "m. On the other\n",
      "hand, ξ+ = 0 and α+\n",
      "i = 0. The case yi −(⟨w,xi⟩+ b) > ϵis symmetric\n",
      "and yields ξ−= 0, ξ+\n",
      "i >0, α+\n",
      "i = C\n",
      "m, and α−\n",
      "i = 0.\n",
      "•Finally, if (⟨w,xi⟩+ b) −yi = ϵ we have ξ−\n",
      "i = 0 and 0 ≤α−\n",
      "i ≤C\n",
      "m, while\n",
      "ξ+ = 0 and α+\n",
      "i = 0. Similarly, when yi −(⟨w,xi⟩+ b) = ϵ we obtain\n",
      "ξ+\n",
      "i = 0, 0 ≤α+\n",
      "i ≤C\n",
      "m, ξ−= 0 and α−\n",
      "i = 0.\n",
      "Note that α+\n",
      "i and α−\n",
      "i are never simultaneously non-zero.\n",
      "7.3.1 Incorporating General Loss Functions\n",
      "Using the same reasoning as in Section 7.1.1 we can deduce from (7.52) that\n",
      "the loss function of support vector regression is given by\n",
      "l(w,x,y ) = max(0,|y−⟨w,x⟩|− ϵ). (7.59)\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Based on the following context, create a quiz with 3 multiple-choice questions about svm. \n",
      "For each question, provide 4 options and indicate the correct answer.\n",
      "\n",
      "Context: As before, we can replace (7.21) by a linear penalty for constraint viola-\n",
      "tion in order to recover (7.5). The quantity log p(yi|xi,θ)\n",
      "maxy̸=yip(y|xi,θ) is sometimes\n",
      "called the log-odds ratio, and the above discussion shows that SVMs can\n",
      "be interpreted as maximizing the log-odds ratio in the exponential family.\n",
      "This interpretation will be developed further when we consider extensions of\n",
      "SVMs to tackle multiclass, multilabel, and structured prediction problems.\n",
      "7.1.3 Specialized Algorithms for Training SVMs\n",
      "The main task in training SVMs boils down to solving (7.9). The m×m\n",
      "matrix H is usually dense and cannot be stored in memory. Decomposition\n",
      "methods are designed to overcome these diﬃculties. The basic idea here\n",
      "is to identify and update a small working set B by solving a small sub-\n",
      "problem at every iteration. Formally, let B ⊂{1,...,m }be the working set\n",
      "and αB be the corresponding sub-vector of α. Deﬁne ¯B = {1,...,m }\\ B\n",
      "\n",
      "Berlin, 1982.\n",
      "[Vap95] , The nature of statistical learning theory, Springer, New York, 1995.\n",
      "[Vap98] , Statistical learning theory, John Wiley and Sons, New York, 1998.\n",
      "[vdG00] S. van de Geer, Empirical processes in M-estimation, Cambridge University\n",
      "Press, 2000.\n",
      "[vdVW96] A. W. van der Vaart and J. A. Wellner, Weak convergence and empirical\n",
      "processes, Springer, 1996.\n",
      "[VGS97] V. Vapnik, S. Golowich, and A. J. Smola, Support vector method for func-\n",
      "tion approximation, regression estimation, and signal processing , Advances in\n",
      "Neural Information Processing Systems 9 (Cambridge, MA) (M. C. Mozer,\n",
      "M. I. Jordan, and T. Petsche, eds.), MIT Press, 1997, pp. 281–287.\n",
      "[Voo01] E. Voorhees, Overview of the TRECT 2001 question answering track ,\n",
      "TREC, 2001.\n",
      "[VS04] S. V. N. Vishwanathan and A. J. Smola, Fast kernels for string and\n",
      "tree matching, Kernel Methods in Computational Biology (Cambridge, MA)\n",
      "(B. Sch¨ olkopf, K. Tsuda, and J. P. Vert, eds.), MIT Press, 2004, pp. 113–130.\n",
      "\n",
      "tree matching, Kernel Methods in Computational Biology (Cambridge, MA)\n",
      "(B. Sch¨ olkopf, K. Tsuda, and J. P. Vert, eds.), MIT Press, 2004, pp. 113–130.\n",
      "[VSV07] S. V. N. Vishwanathan, A. J. Smola, and R. Vidal, Binet-Cauchy kernels\n",
      "on dynamical systems and its application to the analysis of dynamic scenes ,\n",
      "International Journal of Computer Vision 73 (2007), no. 1, 95–119.\n",
      "[Wah97] G. Wahba, Support vector machines, reproducing kernel Hilbert spaces and\n",
      "the randomized GACV, Tech. Report 984, Department of Statistics, University\n",
      "of Wisconsin, Madison, 1997.\n",
      "[Wat64] G. S. Watson, Smooth regression analysis, Sankhya A 26 (1964), 359–372.\n",
      "[Wil98] C. K. I. Williams, Prediction with Gaussian processes: From linear regression\n",
      "to linear prediction and beyond , Learning and Inference in Graphical Models\n",
      "(M. I. Jordan, ed.), Kluwer Academic, 1998, pp. 599–621.\n",
      "\n",
      "Introduction to Machine Learning\n",
      "Alex Smola and S.V.N. Vishwanathan\n",
      "Yahoo! Labs\n",
      "Santa Clara\n",
      "–and–\n",
      "Departments of Statistics and Computer Science\n",
      "Purdue University\n",
      "–and–\n",
      "College of Engineering and Computer Science\n",
      "Australian National University\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Based on the following context, create a quiz with 3 multiple-choice questions about svm. \n",
      "For each question, provide 4 options and indicate the correct answer.\n",
      "\n",
      "Context: As before, we can replace (7.21) by a linear penalty for constraint viola-\n",
      "tion in order to recover (7.5). The quantity log p(yi|xi,θ)\n",
      "maxy̸=yip(y|xi,θ) is sometimes\n",
      "called the log-odds ratio, and the above discussion shows that SVMs can\n",
      "be interpreted as maximizing the log-odds ratio in the exponential family.\n",
      "This interpretation will be developed further when we consider extensions of\n",
      "SVMs to tackle multiclass, multilabel, and structured prediction problems.\n",
      "7.1.3 Specialized Algorithms for Training SVMs\n",
      "The main task in training SVMs boils down to solving (7.9). The m×m\n",
      "matrix H is usually dense and cannot be stored in memory. Decomposition\n",
      "methods are designed to overcome these diﬃculties. The basic idea here\n",
      "is to identify and update a small working set B by solving a small sub-\n",
      "problem at every iteration. Formally, let B ⊂{1,...,m }be the working set\n",
      "and αB be the corresponding sub-vector of α. Deﬁne ¯B = {1,...,m }\\ B\n",
      "\n",
      "Berlin, 1982.\n",
      "[Vap95] , The nature of statistical learning theory, Springer, New York, 1995.\n",
      "[Vap98] , Statistical learning theory, John Wiley and Sons, New York, 1998.\n",
      "[vdG00] S. van de Geer, Empirical processes in M-estimation, Cambridge University\n",
      "Press, 2000.\n",
      "[vdVW96] A. W. van der Vaart and J. A. Wellner, Weak convergence and empirical\n",
      "processes, Springer, 1996.\n",
      "[VGS97] V. Vapnik, S. Golowich, and A. J. Smola, Support vector method for func-\n",
      "tion approximation, regression estimation, and signal processing , Advances in\n",
      "Neural Information Processing Systems 9 (Cambridge, MA) (M. C. Mozer,\n",
      "M. I. Jordan, and T. Petsche, eds.), MIT Press, 1997, pp. 281–287.\n",
      "[Voo01] E. Voorhees, Overview of the TRECT 2001 question answering track ,\n",
      "TREC, 2001.\n",
      "[VS04] S. V. N. Vishwanathan and A. J. Smola, Fast kernels for string and\n",
      "tree matching, Kernel Methods in Computational Biology (Cambridge, MA)\n",
      "(B. Sch¨ olkopf, K. Tsuda, and J. P. Vert, eds.), MIT Press, 2004, pp. 113–130.\n",
      "\n",
      "tree matching, Kernel Methods in Computational Biology (Cambridge, MA)\n",
      "(B. Sch¨ olkopf, K. Tsuda, and J. P. Vert, eds.), MIT Press, 2004, pp. 113–130.\n",
      "[VSV07] S. V. N. Vishwanathan, A. J. Smola, and R. Vidal, Binet-Cauchy kernels\n",
      "on dynamical systems and its application to the analysis of dynamic scenes ,\n",
      "International Journal of Computer Vision 73 (2007), no. 1, 95–119.\n",
      "[Wah97] G. Wahba, Support vector machines, reproducing kernel Hilbert spaces and\n",
      "the randomized GACV, Tech. Report 984, Department of Statistics, University\n",
      "of Wisconsin, Madison, 1997.\n",
      "[Wat64] G. S. Watson, Smooth regression analysis, Sankhya A 26 (1964), 359–372.\n",
      "[Wil98] C. K. I. Williams, Prediction with Gaussian processes: From linear regression\n",
      "to linear prediction and beyond , Learning and Inference in Graphical Models\n",
      "(M. I. Jordan, ed.), Kluwer Academic, 1998, pp. 599–621.\n",
      "\n",
      "Introduction to Machine Learning\n",
      "Alex Smola and S.V.N. Vishwanathan\n",
      "Yahoo! Labs\n",
      "Santa Clara\n",
      "–and–\n",
      "Departments of Statistics and Computer Science\n",
      "Purdue University\n",
      "–and–\n",
      "College of Engineering and Computer Science\n",
      "Australian National University\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Keyboard interruption in main thread... closing server.\n",
      "Killing tunnel 127.0.0.1:7865 <> https://bbfa7916a87b45fcf6.gradio.live\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quiz_data = []\n",
    "current_index = 0\n",
    "user_score = 0\n",
    "\n",
    "def get_relevant_context(query):\n",
    "    docs = retriever.get_relevant_documents(query)\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "    return context\n",
    "\n",
    "def parse_quiz_questions(text):\n",
    "    questions = []\n",
    "    question_blocks = re.split(r'\\n\\s*\\n', text)\n",
    "\n",
    "    for block in question_blocks:\n",
    "        if not block.strip():\n",
    "            continue\n",
    "\n",
    "        question_match = re.search(r'(?:Question\\s*\\d+[:.]\\s*)?(.+?)(?:\\n|\\r\\n|$)', block)\n",
    "        if not question_match:\n",
    "            continue\n",
    "\n",
    "        question = question_match.group(1).strip()\n",
    "        options = []\n",
    "        option_matches = re.findall(r'([A-E])[).]\\s*(.+?)(?:\\n|\\r\\n|$)', block)\n",
    "        if not option_matches or len(option_matches) < 5:\n",
    "            continue\n",
    "\n",
    "        for _, option_text in option_matches:\n",
    "            options.append(option_text.strip())\n",
    "\n",
    "        correct_match = re.search(r'(?:Correct\\s*[Aa]nswer[:.]\\s*)([A-E])', block)\n",
    "        if not correct_match:\n",
    "            continue\n",
    "\n",
    "        correct_letter = correct_match.group(1)\n",
    "        correct_index = ord(correct_letter) - ord('A')\n",
    "\n",
    "        if correct_index < 0 or correct_index >= len(options):\n",
    "            continue\n",
    "\n",
    "        questions.append({\n",
    "            \"question\": question,\n",
    "            \"options\": options,\n",
    "            \"correct_letter\": correct_letter,\n",
    "            \"correct_answer\": options[correct_index]\n",
    "        })\n",
    "\n",
    "    return questions\n",
    "\n",
    "def parse_flashcards(text):\n",
    "    cards = []\n",
    "    matches = re.findall(r'Q:\\s*(.+?)\\s*\\n\\s*A:\\s*(.+?)(?=\\n\\s*Q:|$)', text, re.DOTALL)\n",
    "\n",
    "    for question, answer in matches:\n",
    "        cards.append({\n",
    "            \"question\": question.strip(),\n",
    "            \"answer\": answer.strip()\n",
    "        })\n",
    "\n",
    "    return cards\n",
    "\n",
    "def generate_study_material(query, tool_type):\n",
    "    if not query.strip():\n",
    "        return \"Please enter a topic or question.\"\n",
    "\n",
    "    try:\n",
    "        if tool_type == \"qa\":\n",
    "            result = qa.invoke({\"query\": query})\n",
    "            return result[\"result\"]\n",
    "        else:\n",
    "            context = get_relevant_context(query)\n",
    "\n",
    "            if tool_type == \"quiz\":\n",
    "                result = quiz_chain.invoke({\"query\": query, \"context\": context})\n",
    "                global quiz_data, user_score\n",
    "                user_score = 0\n",
    "                quiz_data = parse_quiz_questions(result[\"text\"])\n",
    "                return display_quiz_question(0)\n",
    "\n",
    "            elif tool_type == \"guide\":\n",
    "                result = guide_chain.invoke({\"query\": query, \"context\": context})\n",
    "                return result[\"text\"]\n",
    "\n",
    "            elif tool_type == \"flashcard\":\n",
    "                result = flashcard_chain.invoke({\"query\": query, \"context\": context})\n",
    "                return format_flashcards(parse_flashcards(result[\"text\"]))\n",
    "\n",
    "            elif tool_type == \"practice\":\n",
    "                result = practice_chain.invoke({\"query\": query, \"context\": context})\n",
    "                return result[\"text\"]\n",
    "\n",
    "            else:\n",
    "                return \"Invalid tool type selected.\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\"\n",
    "\n",
    "def format_flashcards(flashcards):\n",
    "    if not flashcards:\n",
    "        return \"No flashcards could be generated for this topic.\"\n",
    "\n",
    "    formatted_text = \"# Flashcards\\n\\n\"\n",
    "    for i, card in enumerate(flashcards, 1):\n",
    "        formatted_text += f\"### Card {i}\\n\"\n",
    "        formatted_text += f\"*Question:* {card['question']}\\n\\n\"\n",
    "        formatted_text += f\"<details><summary>Click to see answer</summary>\\n\"\n",
    "        formatted_text += f\"{card['answer']}\\n</details>\\n\\n\"\n",
    "\n",
    "    return formatted_text\n",
    "\n",
    "def display_quiz_question(index):\n",
    "    global current_question_index\n",
    "    current_question_index = index\n",
    "\n",
    "    if not quiz_data:\n",
    "        return \"No quiz questions available. Please generate a quiz first.\"\n",
    "\n",
    "    if index >= len(quiz_data):\n",
    "        return f\"Quiz completed! Your score: {user_score}/{len(quiz_data)}\"\n",
    "\n",
    "    question = quiz_data[index]\n",
    "    question_text = f\"## Question {index + 1} of {len(quiz_data)}\\n\\n\"\n",
    "    question_text += f\"{question['question']}\\n\\n\"\n",
    "\n",
    "    for i, option in enumerate(question['options']):\n",
    "        option_letter = chr(65 + i)\n",
    "        question_text += f\"{option_letter}. {option}\\n\"\n",
    "\n",
    "    return question_text\n",
    "\n",
    "def check_answer(answer_letter):\n",
    "    global user_score, current_question_index\n",
    "\n",
    "    if not quiz_data or current_question_index >= len(quiz_data):\n",
    "        return \"No active quiz question.\"\n",
    "\n",
    "    current_question = quiz_data[current_question_index]\n",
    "    correct_letter = current_question[\"correct_letter\"]\n",
    "\n",
    "    result = f\"## Question {current_question_index + 1} Result\\n\\n\"\n",
    "\n",
    "    if answer_letter == correct_letter:\n",
    "        user_score += 1\n",
    "        result += \"✓ Correct!\\n\\n\"\n",
    "    else:\n",
    "        result += f\"✗ Incorrect. The correct answer is {correct_letter}.\\n\\n\"\n",
    "\n",
    "    current_question_index += 1\n",
    "\n",
    "    if current_question_index < len(quiz_data):\n",
    "        result += display_quiz_question(current_question_index)\n",
    "    else:\n",
    "        result += f\"## Quiz Completed!\\n\\nYour final score: {user_score}/{len(quiz_data)}\"\n",
    "\n",
    "    return result\n",
    "\n",
    "def reset_quiz():\n",
    "    global quiz_data, current_question_index, user_score\n",
    "    quiz_data = []\n",
    "    current_question_index = 0\n",
    "    user_score = 0\n",
    "    return \"Quiz reset. Ready to generate a new quiz.\"\n",
    "\n",
    "with gr.Blocks(theme=gr.themes.Soft(primary_hue=\"indigo\")) as demo:\n",
    "    gr.Markdown(\"\"\"\n",
    "    # 📘 Interactive Study Assistant\n",
    "\n",
    "    Ask questions, generate quizzes, study guides, flashcards, and practice problems from the machine learning book.\n",
    "    \"\"\")\n",
    "\n",
    "    with gr.Tab(\"Study Tools\"):\n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=3):\n",
    "                input_text = gr.Textbox(\n",
    "                    label=\"Enter your topic or question\",\n",
    "                    placeholder=\"e.g., Gradient Descent, Neural Networks, Support Vector Machines\",\n",
    "                    lines=3\n",
    "                )\n",
    "                tool_selector = gr.Radio(\n",
    "                    choices=[\"qa\", \"guide\", \"flashcard\", \"practice\"],\n",
    "                    value=\"qa\",\n",
    "                    label=\"Select Study Tool\",\n",
    "                    info=\"Choose the type of study material you want to generate\"\n",
    "                )\n",
    "                with gr.Row():\n",
    "                    generate_btn = gr.Button(\"Generate\", variant=\"primary\")\n",
    "                    clear_btn = gr.Button(\"Clear\")\n",
    "\n",
    "            with gr.Column(scale=4):\n",
    "                output_text = gr.Markdown(label=\"Generated Content\")\n",
    "\n",
    "    with gr.Tab(\"Quiz\"):\n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=1):\n",
    "                quiz_topic = gr.Textbox(\n",
    "                    label=\"Quiz Topic\",\n",
    "                    placeholder=\"Enter a topic for your quiz\",\n",
    "                    lines=2\n",
    "                )\n",
    "                quiz_btn = gr.Button(\"Generate Quiz\", variant=\"primary\")\n",
    "                reset_btn = gr.Button(\"Reset Quiz\")\n",
    "\n",
    "            with gr.Column(scale=2):\n",
    "                quiz_display = gr.Markdown(label=\"Quiz Question\")\n",
    "\n",
    "        with gr.Row():\n",
    "            answer_options = gr.Radio(\n",
    "                choices=[\"A\", \"B\", \"C\", \"D\", \"E\"],\n",
    "                label=\"Your Answer\"\n",
    "            )\n",
    "            submit_answer_btn = gr.Button(\"Submit Answer\", variant=\"secondary\")\n",
    "\n",
    "    with gr.Tab(\"About\"):\n",
    "        gr.Markdown(\"\"\"\n",
    "        ## About This Tool\n",
    "\n",
    "        This interactive study assistant helps you learn machine learning concepts from the textbook. You can:\n",
    "\n",
    "        - *Ask questions* about specific concepts\n",
    "        - *Generate quizzes* to test your knowledge\n",
    "        - *Create study guides* for revision\n",
    "        - *Make flashcards* for active recall practice\n",
    "        - *Get practice problems* to apply your learning\n",
    "\n",
    "        Use the different tabs to access each feature.\n",
    "        \"\"\")\n",
    "\n",
    "    generate_btn.click(\n",
    "        fn=generate_study_material,\n",
    "        inputs=[input_text, tool_selector],\n",
    "        outputs=output_text\n",
    "    )\n",
    "\n",
    "    clear_btn.click(\n",
    "        fn=lambda: \"\",\n",
    "        inputs=None,\n",
    "        outputs=output_text\n",
    "    )\n",
    "\n",
    "    quiz_btn.click(\n",
    "        fn=lambda topic: generate_study_material(topic, \"quiz\"),\n",
    "        inputs=quiz_topic,\n",
    "        outputs=quiz_display\n",
    "    )\n",
    "\n",
    "    reset_btn.click(\n",
    "        fn=reset_quiz,\n",
    "        inputs=None,\n",
    "        outputs=quiz_display\n",
    "    )\n",
    "\n",
    "    submit_answer_btn.click(\n",
    "        fn=check_answer,\n",
    "        inputs=answer_options,\n",
    "        outputs=quiz_display\n",
    "    )\n",
    "\n",
    "demo.launch(share=True, debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25ea3ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing quiz functionality...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\udhay\\AppData\\Local\\Temp\\ipykernel_5652\\3307634288.py:4: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = retriever.get_relevant_documents(input_text)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Based on the following context, create a quiz with 3 multiple-choice questions about Gradient Descent. \n",
      "For each question, provide 4 options and indicate the correct answer.\n",
      "\n",
      "Context: learning applications, coordinate descent is often used because a) the cost\n",
      "per iteration is very low and b) the speed of convergence may be acceptable\n",
      "especially if the variables are loosely coupled.\n",
      "3.2.3 Gradient Descent\n",
      "Gradient descent (also widely known as steepest descent) is an optimization\n",
      "technique for minimizing multidimensional smooth convex objective func-\n",
      "tions of the form J : Rn →R. The basic idea is as follows: Given a location\n",
      "\n",
      "136 3 Optimization\n",
      "Stochastic gradient-based methods, by contrast, work with gradient esti-\n",
      "mates obtained from small subsamples (mini-batches) of training data. This\n",
      "can greatly reduce computational requirements: on large, redundant data\n",
      "sets, simple stochastic gradient descent routinely outperforms sophisticated\n",
      "second-order batch methods by orders of magnitude.\n",
      "The key idea here is that J(w) is replaced by an instantaneous estimate\n",
      "Jt which is computed from a mini-batch of size k comprising of a subset of\n",
      "points (xt\n",
      "i,yt\n",
      "i) with i= 1,...,k drawn from the dataset:\n",
      "Jt(w) = λΩ(w) + 1\n",
      "k\n",
      "k∑\n",
      "i=1\n",
      "l(w,xt\n",
      "i,yt\n",
      "i). (3.129)\n",
      "Setting k = 1 obtains an algorithm which processes data points as they\n",
      "arrive.\n",
      "3.4.1 Stochastic Gradient Descent\n",
      "Perhaps the simplest stochastic optimization algorithm is Stochastic Gradi-\n",
      "ent Descent (SGD). The parameter update of SGD takes the form:\n",
      "wt+1 = wt −ηt∇Jt(wt). (3.130)\n",
      "If Jt is not diﬀerentiable, then one can choose an arbitrary subgradient from\n",
      "\n",
      "106 3 Optimization\n",
      "Algorithm 3.2 Gradient Descent\n",
      "1: Input: Initial point w0, gradient norm tolerance ϵ\n",
      "2: Set t= 0\n",
      "3: while ∥∇J(wt)∥≥ ϵ do\n",
      "4: wt+1 = wt −ηt∇J(wt)\n",
      "5: t= t+ 1\n",
      "6: end while\n",
      "7: Return: wt\n",
      "Decaying Stepsize: Instead of performing a line search at every itera-\n",
      "tion, one can use a stepsize which decays according to a ﬁxed schedule, for\n",
      "example, ηt = 1/\n",
      "√\n",
      "t. In Section 3.2.4 we will discuss the decay schedule and\n",
      "convergence rates of a generalized version of gradient descent.\n",
      "Fixed Stepsize: Suppose J has a Lipschitz continuous gradient with mod-\n",
      "ulus L. Using (3.22) and the gradient descent update wt+1 = wt−ηt∇J(wt)\n",
      "one can write\n",
      "J(wt+1) ≤J(wt) + ⟨∇J(wt),wt+1 −wt⟩+ L\n",
      "2 ∥wt+1 −wt∥ (3.44)\n",
      "= J(wt) −ηt∥∇J(wt)∥2 + Lη2\n",
      "t\n",
      "2 ∥∇J(wt)∥2 . (3.45)\n",
      "Minimizing (3.45) as a function of ηt clearly shows that the upper bound on\n",
      "J(wt+1) is minimized when we set ηt = 1\n",
      "L, which is the ﬁxed stepsize rule.\n",
      "Theorem 3.12 Suppose J has a Lipschitz continuous gradient with modu-\n",
      "\n",
      "number of diﬀerent algorithms we described below can be understood as\n",
      "explicitly or implicitly changing the condition number of the problem to\n",
      "accelerate convergence.\n",
      "3.2.4 Mirror Descent\n",
      "One way to motivate gradient descent is to use the following quadratic ap-\n",
      "proximation of the objective function\n",
      "Qt(w) := J(wt) + ⟨∇J(wt),w −wt⟩+ 1\n",
      "2(w−wt)⊤(w−wt), (3.49)\n",
      "where, as in the previous section, ∇J(·) denotes the gradient of J. Mini-\n",
      "mizing this quadratic model at every iteration entails taking gradients with\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Question 1: What is another name for Gradient Descent?\n",
      "A) Coordinate Descent\n",
      "B) Stochastic Gradient Descent\n",
      "C) Steepest Descent\n",
      "D) Mirror Descent\n",
      "Correct Answer: C) Steepest Descent\n",
      "\n",
      "Question 2: What is the parameter update form of Stochastic Gradient Descent (SGD)?\n",
      "A) wt+1 = wt + ηt∇Jt(wt)\n",
      "B) wt+1 = wt - ηt∇Jt(wt)\n",
      "C) wt+1 = wt / ηt∇Jt(wt)\n",
      "D) wt+1 = wt * ηt∇Jt(wt)\n",
      "Correct Answer: B) wt+1 = wt - ηt∇Jt(wt)\n",
      "\n",
      "Question 3: What is the condition for the termination of the Gradient Descent algorithm?\n",
      "A) When the gradient norm tolerance is less than a threshold value\n",
      "B) When the gradient norm tolerance is greater than a threshold value\n",
      "C) When the number of iterations exceeds a certain limit\n",
      "D) When the gradient norm tolerance is exactly equal to a threshold value\n",
      "Correct Answer: A) When the gradient norm tolerance is less than a threshold value\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing quiz functionality...\")\n",
    "test_result = use_study_tools(\"Gradient Descent\", \"quiz\")\n",
    "print(test_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
